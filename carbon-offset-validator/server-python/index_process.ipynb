{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process country policy and carbon policy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from fastapi import UploadFile\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "# # A collection in ChromaDB is a logical grouping of embeddings, documents, and metadataâ€”like a table in a database.\n",
    "# # Create a collection for all policys, and another one for all PDD (for similarity search)\n",
    "chroma_collection = chroma_client.create_collection(\"policy_vcm\") #f\"{document_name}_{uuid.uuid4()}\"\n",
    "# # vector_store is a LlamaIndex wrapper around the ChromaDB collection, providing an interface to interact with it (e.g., adding nodes, querying).\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# define embedding model\n",
    "model_name = \"models/embedding-001\"\n",
    "embed_model = GeminiEmbedding(model_name = model_name, \n",
    "                            api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "#NOTE MAKE THE REAL PATH!\n",
    "# also need to process a list of files\n",
    "# and create metadata field to indicate file names\n",
    "\n",
    "def get_file_paths(folder_path):\n",
    "    \"\"\"Get all file paths in a folder recursively.\"\"\"\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    return file_paths\n",
    "\n",
    "# get the folder\n",
    "file_folder_path = os.path.join(os.getcwd(), \"policy_vcm_docs\")  # Adjust this to your actual path\n",
    "\n",
    "# Get all file paths in the policy_doc folder\n",
    "file_paths = get_file_paths(file_folder_path)\n",
    "\n",
    "# Use DoclingReader to load the data\n",
    "reader = DoclingReader()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    document = reader.load_data(file_path)\n",
    "    \n",
    "    # Add file name to metadata for each document\n",
    "    for doc in documents:\n",
    "        # If documents is a list of Document objects\n",
    "        if isinstance(doc, Document):\n",
    "            if doc.metadata is None:\n",
    "                doc.metadata = {}\n",
    "            doc.metadata[\"file_name\"] = file_name\n",
    "        # If it's a dict or another structure, adjust accordingly\n",
    "        elif hasattr(doc, 'metadata'):\n",
    "            if doc.metadata is None:\n",
    "                doc.metadata = {}\n",
    "            doc.metadata[\"file_name\"] = file_name\n",
    "            \n",
    "    # create ingestion pipeline to define splitter and embed model \n",
    "    # I also want to add the doc title to metadata\n",
    "    \n",
    "    # how to extract more metadata???\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(chunk_size=500, chunk_overlap=50),  # Match your project chunking\n",
    "            TitleExtractor(),\n",
    "            embed_model\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the index\n",
    "    # node_parser = MarkdownNodeParser()\n",
    "    # nodes = await pipeline.run(documents=documents)\n",
    "    nodes = pipeline.run(documents=document)\n",
    "    index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        # transformations=[node_parser],\n",
    "        embed_model=embed_model,\n",
    "        vector_store=vector_store\n",
    "    )\n",
    "\n",
    "# persist the index\n",
    "# storage_dir = os.path.join(os.getcwd(), \"storage\", f\"{document_name}_{uuid.uuid4()}\")\n",
    "# os.makedirs(storage_dir, exist_ok=True)\n",
    "# index.storage_context.persist(persist_dir=storage_dir)\n",
    "\n",
    "\n",
    "# To load the storage later:\n",
    "# from llama_index.core import load_index_from_storage\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "# policy_index = load_index_from_storage(storage_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(pinecone.Index(\"quickstart\"))\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carbonoff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
