{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "# check current directory\n",
    "print(os.getcwd())\n",
    "# go above onre directory\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 0: Set up embedding model & LLM\n",
    "- Step 1: Read PDFs with Docling and assign doc_id\n",
    "- Step 2: Set up ChromaDB and index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Set up embedding model & LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_from_colab_or_os(key):\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        try:\n",
    "            return userdata.get(key)\n",
    "        except userdata.SecretNotFoundError:\n",
    "            pass\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return os.getenv(key)\n",
    "\n",
    "load_dotenv()\n",
    "EMBED_MODEL = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "GEN_MODEL = HuggingFaceInferenceAPI(\n",
    "    token=get_env_from_colab_or_os(\"HF_TOKEN\"),\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"data/pdd/674_PROJ_DESC_674_15MAY2011.pdf\"\n",
    "# QUERY = \"Evaluate this project based on the ICVCM CCPs standard. Describe in detail the additionality methodology and any data to support the claim.\"\n",
    "QUERY = \"Describe in detail the additionality methodology and any data to support the claim.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse docuemnt with docling\n",
    "reader = DoclingReader()\n",
    "# node_parser = MarkdownNodeParser()\n",
    "documents = reader.load_data(SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = chroma_client.create_collection(\"pdd_collection\")\n",
    "# may need to change this chroma collection to load chroma instead of create collection\n",
    "# Set up ChromaVectorStore\n",
    "vector_store = ChromaVectorStore(chroma_collection = chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# build index with embeddings\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context = storage_context,\n",
    "    embed_model = EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up chromaDB space\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"pdd_collection\")\n",
    "vector_store = ChromaVectorStore(chroma_collection = chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the existing index\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=EMBED_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DoclingReader()\n",
    "new_file_paths_proj = [] # add the new file paths here\n",
    "new_file_paths_stdrd = [] # add the new file paths here\n",
    "\n",
    "# a function to add metadata to the document\n",
    "\n",
    "def add_docs(list_of_paths: list, doc_type=\"project\"):\n",
    "    \"\"\"Process documents with Docling, add metadata based on doc_type (can be either \"project\" or \"standard\") and insert nodes into the index.\n",
    "\n",
    "    Args:\n",
    "        list_of_paths (list): List of PDF file paths to be processed.\n",
    "        doc_type (str, optional): Type of PDF, either \"project\" or \"standard\". Defaults to \"project\".\n",
    "    \"\"\"\n",
    "    new_documents = []\n",
    "    for file_path in list_of_paths:\n",
    "        raw_docs = reader.load_data(file_paths=[file_path])\n",
    "        # Extract the project ID from the file path\n",
    "        item_id = file_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "        for doc in raw_docs:\n",
    "            if doc_type == \"project\":\n",
    "                doc.metadata[\"proj_id\"] = item_id\n",
    "            else:\n",
    "                doc.metadata[\"stdrd_id\"] = item_id\n",
    "            doc.metadata[\"doc_type\"] = doc_type\n",
    "        new_documents.extend(raw_docs)\n",
    "    index.insert_nodes([doc.to_node() for doc in new_documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query a specific doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "# Define filter for a specific document\n",
    "filters = MetadataFilters(filters=[\n",
    "    ExactMatchFilter(key=\"doc_id\", value=\"doc_005\")  # Target doc_005\n",
    "])\n",
    "query_engine = index.as_query_engine(llm=GEN_MODEL, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What do my documents say about additionality?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mar 10th todo: agentic rag codes \n",
    "- each document has its own code...\n",
    "- go back to the tutorial and check code\n",
    "\n",
    "- PDD and project standards should be in the same DB but with different metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carbonoff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
